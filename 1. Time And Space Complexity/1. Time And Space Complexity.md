# Complete Guide: Time & Space Complexity Analysis

## ğŸ¯ Step-by-Step Process to Find Complexity

### **STEP 1: Identify the Input Size**
- Usually denoted as `n` (array size, string length, etc.)
- Sometimes multiple inputs: `n`, `m`, `k`

### **STEP 2: Count Operations/Memory Usage**

---

## ğŸ” TIME COMPLEXITY TRICKS

### **Trick 1: Loop Analysis**

```python
# Single Loop = O(n)
for i in range(n):
    print(i)  # O(1) operation
# Total: O(n)

# Nested Loops = Multiply
for i in range(n):      # O(n)
    for j in range(m):  # O(m)
        print(i, j)     # O(1)
# Total: O(n Ã— m)

# Triple Nested = O(nÂ³)
for i in range(n):
    for j in range(n):
        for k in range(n):
            print(i, j, k)
# Total: O(nÂ³)
```

### **Trick 2: Loop Dependency Analysis**

```python
# Dependent on outer loop variable
for i in range(n):
    for j in range(i):  # j goes from 0 to i-1
        print(i, j)

# Mathematical calculation:
# i=0: 0 iterations
# i=1: 1 iteration  
# i=2: 2 iterations
# ...
# i=n-1: n-1 iterations
# Total = 0+1+2+...+(n-1) = n(n-1)/2 = O(nÂ²)
```

### **Trick 3: Logarithmic Patterns**

```python
# Dividing by 2 each time = O(log n)
i = n
while i > 1:
    print(i)
    i = i // 2  # Key: dividing by constant

# Binary Search Pattern
def binary_search(arr, target):
    left, right = 0, len(arr) - 1
    while left <= right:  # log n iterations
        mid = (left + right) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
    return -1
# Time: O(log n)
```

### **Trick 4: Recursive Complexity**

```python
# Master Theorem: T(n) = aT(n/b) + f(n)
# where a = number of recursive calls
#       b = input size reduction factor
#       f(n) = work done outside recursion

# Example 1: Factorial
def factorial(n):
    if n <= 1:
        return 1
    return n * factorial(n-1)  # 1 call, reduces by 1
# T(n) = T(n-1) + O(1) = O(n)

# Example 2: Binary Tree Traversal
def traverse(node):
    if not node:
        return
    traverse(node.left)   # 1 call
    traverse(node.right)  # 1 call
# T(n) = 2T(n/2) + O(1) = O(n)

# Example 3: Merge Sort
def merge_sort(arr):
    if len(arr) <= 1:
        return arr
    mid = len(arr) // 2
    left = merge_sort(arr[:mid])    # T(n/2)
    right = merge_sort(arr[mid:])   # T(n/2)
    return merge(left, right)       # O(n)
# T(n) = 2T(n/2) + O(n) = O(n log n)
```

---

## ğŸ’¾ SPACE COMPLEXITY TRICKS

### **Trick 1: Variable Counting**

```python
def example1(n):
    a = 5        # O(1)
    b = 10       # O(1)
    c = a + b    # O(1)
    return c
# Space: O(1) - constant variables

def example2(n):
    arr = [0] * n  # O(n) space
    return sum(arr)
# Space: O(n) - array of size n
```

### **Trick 2: Recursive Space (Call Stack)**

```python
def recursive_func(n):
    if n <= 0:
        return 0
    return n + recursive_func(n-1)
# Each call uses stack space
# Maximum depth = n
# Space: O(n) due to call stack
```

### **Trick 3: Auxiliary Data Structures**

```python
def has_duplicates(arr):
    seen = set()  # Extra space
    for num in arr:
        if num in seen:
            return True
        seen.add(num)  # Worst case: all elements unique
    return False
# Space: O(n) for the set
```

---

## ğŸ§® DETAILED MATHEMATICAL ANALYSIS

### **Essential Summation Formulas:**

#### **1. Arithmetic Progression (AP):**
```
Formula: Î£(i=1 to n) i = 1 + 2 + 3 + ... + n = n(n+1)/2

Mathematical Proof:
S = 1 + 2 + 3 + ... + n
S = n + (n-1) + (n-2) + ... + 1
2S = (n+1) + (n+1) + ... + (n+1)  [n times]
2S = n(n+1)
S = n(n+1)/2 = O(nÂ²)

Code Example:
for i in range(1, n+1):
    for j in range(1, i+1):  # j runs i times
        print(j)
# Total operations = 1 + 2 + 3 + ... + n = n(n+1)/2 = O(nÂ²)
```

#### **2. Geometric Progression (GP):**
```
Formula: Î£(i=0 to n) 2^i = 1 + 2 + 4 + 8 + ... + 2^n = 2^(n+1) - 1

Mathematical Proof:
S = 1 + 2 + 4 + ... + 2^n
2S = 2 + 4 + 8 + ... + 2^(n+1)
2S - S = 2^(n+1) - 1
S = 2^(n+1) - 1 = O(2^n)

Code Example:
operations = 0
i = 1
while i <= n:
    operations += i  # Add current level operations
    i *= 2          # Double each time
# Total = 1 + 2 + 4 + ... â‰ˆ O(2^log n) = O(n)
```

#### **3. Harmonic Series:**
```
Formula: Î£(i=1 to n) 1/i = 1 + 1/2 + 1/3 + ... + 1/n â‰ˆ ln(n) = O(log n)

Code Example:
for i in range(1, n+1):
    for j in range(i, n+1, i):  # j = i, 2i, 3i, ... (n/i times)
        print(j)
# Total = n/1 + n/2 + n/3 + ... + n/n = n(1 + 1/2 + 1/3 + ... + 1/n) = O(n log n)
```

### **Advanced Mathematical Patterns:**

#### **4. Square Summation:**
```
Formula: Î£(i=1 to n) iÂ² = 1Â² + 2Â² + 3Â² + ... + nÂ² = n(n+1)(2n+1)/6 = O(nÂ³)

Code Example:
for i in range(1, n+1):
    for j in range(1, i+1):
        for k in range(1, j+1):
            print(k)
# Complexity analysis: Î£(i=1 to n) Î£(j=1 to i) j = Î£(i=1 to n) i(i+1)/2 â‰ˆ O(nÂ³)
```

#### **5. Logarithmic Summation:**
```
Formula: Î£(i=1 to n) log(i) = log(1) + log(2) + ... + log(n) = log(n!) â‰ˆ n log(n) - n = O(n log n)

Code Example:
def complex_algorithm(n):
    total = 0
    for i in range(1, n+1):
        j = i
        while j > 1:
            total += 1
            j //= 2  # Each inner loop runs log(i) times
    return total
# Total = log(1) + log(2) + ... + log(n) = O(n log n)
```

### **Master Theorem (Complete Analysis):**

```
For recurrence relation: T(n) = aT(n/b) + f(n)
where: a â‰¥ 1, b > 1, f(n) > 0

Let c = log_b(a)

Case 1: If f(n) = O(n^d) where d < c
        Then T(n) = O(n^c)

Case 2: If f(n) = O(n^c * log^k(n)) where k â‰¥ 0
        Then T(n) = O(n^c * log^(k+1)(n))

Case 3: If f(n) = O(n^d) where d > c and af(n/b) â‰¤ cf(n) for some c < 1
        Then T(n) = O(f(n))
```

#### **Master Theorem Examples:**

```python
# Example 1: Binary Search
def binary_search(arr, target, left, right):
    if left > right:
        return -1
    mid = (left + right) // 2
    if arr[mid] == target:
        return mid
    elif arr[mid] < target:
        return binary_search(arr, target, mid+1, right)  # T(n/2)
    else:
        return binary_search(arr, target, left, mid-1)   # T(n/2)

# Recurrence: T(n) = T(n/2) + O(1)
# a=1, b=2, f(n)=O(1)=O(n^0)
# c = log_2(1) = 0
# Since f(n) = O(n^0) and d = c = 0 â†’ Case 2
# T(n) = O(n^0 * log n) = O(log n)
```

```python
# Example 2: Merge Sort
def merge_sort(arr):
    if len(arr) <= 1:
        return arr
    mid = len(arr) // 2
    left = merge_sort(arr[:mid])     # T(n/2)
    right = merge_sort(arr[mid:])    # T(n/2)
    return merge(left, right)        # O(n)

# Recurrence: T(n) = 2T(n/2) + O(n)
# a=2, b=2, f(n)=O(n)=O(n^1)
# c = log_2(2) = 1
# Since f(n) = O(n^1) and d = c = 1 â†’ Case 2
# T(n) = O(n^1 * log n) = O(n log n)
```

### **Asymptotic Analysis Rules:**

#### **Big-O Properties:**
```
1. Constant Rule: O(c) = O(1) for any constant c
2. Sum Rule: O(f(n)) + O(g(n)) = O(max(f(n), g(n)))
3. Product Rule: O(f(n)) * O(g(n)) = O(f(n) * g(n))
4. Transitivity: If f(n) = O(g(n)) and g(n) = O(h(n)), then f(n) = O(h(n))
5. Polynomial Rule: O(n^a) + O(n^b) = O(n^max(a,b))
```

#### **Mathematical Limits for Complexity:**
```
Limit Method:
If lim(nâ†’âˆ) f(n)/g(n) = c where c is finite and positive, then f(n) = Î˜(g(n))
If lim(nâ†’âˆ) f(n)/g(n) = 0, then f(n) = O(g(n))
If lim(nâ†’âˆ) f(n)/g(n) = âˆ, then f(n) = Î©(g(n))

Example:
f(n) = 3nÂ² + 2n + 1
g(n) = nÂ²
lim(nâ†’âˆ) (3nÂ² + 2n + 1)/nÂ² = lim(nâ†’âˆ) (3 + 2/n + 1/nÂ²) = 3
Therefore, f(n) = Î˜(nÂ²)
```

### **Probability & Amortized Analysis:**

#### **Expected Time Complexity:**
```python
# Randomized Quick Sort
def randomized_quicksort(arr):
    # Pivot chosen randomly
    # Best case: Always median â†’ O(n log n)
    # Worst case: Always min/max â†’ O(nÂ²)
    # Expected case: E[T(n)] = O(n log n)
    
# Mathematical expectation:
# E[T(n)] = E[2T(n/2) + n] = 2E[T(n/2)] + n = O(n log n)
```

#### **Amortized Analysis:**
```python
# Dynamic Array (ArrayList)
class DynamicArray:
    def append(self, item):
        if self.size == self.capacity:
            self.resize()  # O(n) - happens rarely
        self.array[self.size] = item  # O(1)
        self.size += 1

# Amortized analysis:
# n insertions: n-1 are O(1), few are O(n)
# Total cost = n + (cost of all resizes)
# Resizes happen at sizes: 1, 2, 4, 8, ..., n
# Total resize cost = 1 + 2 + 4 + ... + n = 2n - 1 = O(n)
# Amortized cost per operation = O(n)/n = O(1)
```

### **Complex Recurrence Solutions:**

#### **Substitution Method:**
```
Example: T(n) = 2T(n/2) + nÂ²

Step 1: Guess T(n) = O(nÂ²)
Step 2: Prove by induction
Assume T(k) â‰¤ ckÂ² for all k < n
T(n) = 2T(n/2) + nÂ²
     â‰¤ 2c(n/2)Â² + nÂ²
     = 2c(nÂ²/4) + nÂ²
     = cnÂ²/2 + nÂ²
     = nÂ²(c/2 + 1)
     â‰¤ cnÂ² if c â‰¥ 2

Therefore, T(n) = O(nÂ²)
```

### **Mathematical Proof Techniques:**

#### **1. Direct Counting Method:**
```python
def nested_loops_analysis(n):
    count = 0
    for i in range(n):           # i: 0 to n-1
        for j in range(i, n):    # j: i to n-1
            count += 1
    return count

# Mathematical Analysis:
# When i=0: j runs from 0 to n-1 â†’ n operations
# When i=1: j runs from 1 to n-1 â†’ n-1 operations  
# When i=2: j runs from 2 to n-1 â†’ n-2 operations
# ...
# When i=n-1: j runs from n-1 to n-1 â†’ 1 operation

# Total = n + (n-1) + (n-2) + ... + 1
# Using AP formula: Sum = n(n+1)/2 = O(nÂ²)
```

#### **2. Integral Approximation:**
```python
# For continuous approximation of sums
def logarithmic_complexity(n):
    total = 0
    for i in range(1, n+1):
        j = i
        while j >= 1:
            total += 1
            j //= 2  # Each inner loop runs âŒŠlogâ‚‚(i)âŒ‹ + 1 times
    return total

# Mathematical Analysis:
# Total = Î£(i=1 to n) âŒŠlogâ‚‚(i)âŒ‹
# Approximation: âˆ«â‚â¿ logâ‚‚(x) dx = [x logâ‚‚(x) - x/ln(2)]â‚â¿
# = n logâ‚‚(n) - n/ln(2) + 1/ln(2) â‰ˆ O(n log n)
```

#### **3. Generating Functions:**
```python
# For complex recurrence relations
def fibonacci_analysis(n):
    # T(n) = T(n-1) + T(n-2) + O(1)
    # Characteristic equation: xÂ² = x + 1
    # xÂ² - x - 1 = 0
    # x = (1 Â± âˆš5)/2
    
    # Ï† = (1 + âˆš5)/2 â‰ˆ 1.618 (golden ratio)
    # Ïˆ = (1 - âˆš5)/2 â‰ˆ -0.618
    
    # Solution: T(n) = AÂ·Ï†â¿ + BÂ·Ïˆâ¿
    # Since |Ïˆ| < 1, Ïˆâ¿ â†’ 0 as n â†’ âˆ
    # Therefore: T(n) = O(Ï†â¿) = O(1.618â¿)
    
    if n <= 1:
        return n
    return fibonacci_analysis(n-1) + fibonacci_analysis(n-2)
```

### **Probabilistic Analysis:**

#### **Expected Value Calculations:**
```python
# Randomized Algorithm Analysis
import random

def randomized_select(arr, k):
    # Expected time to find kth smallest element
    # E[T(n)] = E[partition cost] + E[recursive call cost]
    
    if len(arr) == 1:
        return arr[0]
    
    pivot = random.choice(arr)  # Random pivot selection
    # Probability analysis:
    # P(good split) = 1/2 (pivot in middle 50%)
    # P(bad split) = 1/2
    
    # Expected recurrence:
    # E[T(n)] = n + (1/2)E[T(n/2)] + (1/2)E[T(3n/4)]
    # Solving: E[T(n)] = O(n)
    
    # Mathematical proof uses linearity of expectation
    pass
```

#### **Amortized Analysis Methods:**

```python
# 1. Aggregate Method
class StackWithMultipop:
    def __init__(self):
        self.stack = []
    
    def push(self, x):
        self.stack.append(x)  # O(1)
    
    def pop(self):
        if self.stack:
            return self.stack.pop()  # O(1)
    
    def multipop(self, k):
        for _ in range(min(k, len(self.stack))):
            self.pop()  # O(min(k, stack_size))

# Analysis: n operations total
# Each element pushed at most once, popped at most once
# Total cost = O(n), Amortized cost per operation = O(1)

# 2. Accounting Method
class DynamicArray:
    def __init__(self):
        self.capacity = 1
        self.size = 0
        self.array = [None] * self.capacity
    
    def append(self, item):
        if self.size == self.capacity:
            # Charge 3 units: 1 for current, 2 for future moves
            self.resize()  # Actual cost: O(n)
        
        self.array[self.size] = item
        self.size += 1

# Accounting: Each insertion pays 3 units
# 1 unit for immediate insertion
# 2 units saved for future resize cost
# When resize happens, saved credits pay for it
# Amortized cost = O(1) per operation

# 3. Potential Method
def potential_function(array):
    # Î¦(D) = 2 * size - capacity
    # Î”Î¦ = Î¦(after) - Î¦(before)
    # Amortized cost = actual cost + Î”Î¦
    return 2 * array.size - array.capacity
```

### **Advanced Complexity Classes:**

#### **Complexity Hierarchy:**
```
O(1) âŠ‚ O(log log n) âŠ‚ O(log n) âŠ‚ O(n^(1/3)) âŠ‚ O(n^(1/2)) âŠ‚ O(n/log n) âŠ‚ 
O(n) âŠ‚ O(n log n) âŠ‚ O(n^2) âŠ‚ O(n^3) âŠ‚ O(2^n) âŠ‚ O(n!) âŠ‚ O(n^n)

Mathematical relationships:
- log n grows slower than any polynomial: lim(nâ†’âˆ) log n / n^Îµ = 0 for any Îµ > 0
- Polynomial grows slower than exponential: lim(nâ†’âˆ) n^k / 2^n = 0 for any k
- Exponential grows slower than factorial: lim(nâ†’âˆ) 2^n / n! = 0
```

#### **Space-Time Tradeoffs:**
```python
# Example: String Matching
def naive_string_match(text, pattern):
    # Time: O(nm), Space: O(1)
    n, m = len(text), len(pattern)
    for i in range(n - m + 1):
        if text[i:i+m] == pattern:
            return i
    return -1

def kmp_string_match(text, pattern):
    # Time: O(n+m), Space: O(m)
    # Preprocessing creates failure function using O(m) space
    # But reduces time complexity significantly
    pass

# Mathematical relationship:
# Space Ã— Time â‰¥ Î©(problem_size)
# Sometimes we can trade space for time or vice versa
```

### **Practical Mathematical Tools:**

#### **Stirling's Approximation:**
```
n! â‰ˆ âˆš(2Ï€n) * (n/e)^n

Applications:
- Permutation algorithms: P(n,r) = n!/(n-r)! 
- Combination algorithms: C(n,r) = n!/(r!(n-r)!)
- Analysis of algorithms generating all permutations

Example:
def all_permutations(arr):
    # Generates n! permutations
    # Time complexity: O(n! Ã— n) to generate and print
    # Using Stirling: O(âˆš(2Ï€n) Ã— (n/e)^n Ã— n)
    pass
```

#### **Change of Base Formula:**
```
log_a(n) = log_b(n) / log_b(a)

Applications:
- Binary search variants
- Tree height calculations
- Complexity conversions

Example: Binary tree vs Ternary tree
- Binary tree height: O(logâ‚‚ n)
- Ternary tree height: O(logâ‚ƒ n) = O(logâ‚‚ n / logâ‚‚ 3) â‰ˆ O(0.63 Ã— logâ‚‚ n)
```

### **Example 1: Bubble Sort**
```python
def bubble_sort(arr):
    n = len(arr)
    for i in range(n):           # n iterations
        for j in range(n-1-i):   # (n-1), (n-2), ..., 1 iterations
            if arr[j] > arr[j+1]:
                arr[j], arr[j+1] = arr[j+1], arr[j]  # O(1)

# Time Analysis:
# Total comparisons = (n-1) + (n-2) + ... + 1 = n(n-1)/2 = O(nÂ²)
# Space Analysis: O(1) - only using constant extra space
```

### **Example 2: Quick Sort**
```python
def quicksort(arr, low, high):
    if low < high:
        pi = partition(arr, low, high)  # O(n)
        quicksort(arr, low, pi-1)       # T(n/2) average
        quicksort(arr, pi+1, high)      # T(n/2) average

# Time Analysis:
# Best/Average: T(n) = 2T(n/2) + O(n) = O(n log n)
# Worst: T(n) = T(n-1) + O(n) = O(nÂ²)
# Space Analysis: O(log n) average (recursion stack)
```

### **Example 3: Matrix Multiplication**
```python
def matrix_multiply(A, B):
    n = len(A)
    C = [[0] * n for _ in range(n)]  # O(nÂ²) space
    
    for i in range(n):      # n iterations
        for j in range(n):  # n iterations  
            for k in range(n):  # n iterations
                C[i][j] += A[i][k] * B[k][j]  # O(1)
    
    return C

# Time: O(nÂ³) - three nested loops
# Space: O(nÂ²) - result matrix
```

---

## ğŸ”§ QUICK IDENTIFICATION RULES

### **Time Complexity Patterns:**
- **One loop:** O(n)
- **Two nested loops:** O(nÂ²)
- **Dividing in half:** O(log n)
- **Tree traversal:** O(n)
- **Sorting:** O(n log n) typically
- **All permutations:** O(n!)
- **All subsets:** O(2â¿)

### **Space Complexity Patterns:**
- **Few variables:** O(1)
- **One array:** O(n)
- **2D array:** O(nÂ²)
- **Recursion depth:** O(depth)
- **Memoization:** O(unique states)

---

## ğŸ¯ STEP-BY-STEP ANALYSIS METHOD

1. **Identify input size:** What grows? (usually n)
2. **Find all loops:** Count nesting levels
3. **Check loop bounds:** Constant or dependent on n?
4. **Analyze recursive calls:** How many? How deep?
5. **Count extra space:** Arrays, recursion stack, etc.
6. **Apply mathematical formulas:** Sum series if needed
7. **Take the dominant term:** Drop constants and lower terms

### **Final Rules:**
- **Drop constants:** O(2n) â†’ O(n)
- **Drop lower terms:** O(nÂ² + n) â†’ O(nÂ²)  
- **Worst case matters:** Always consider the worst scenario
